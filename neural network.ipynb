{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ARqJytb_sgIz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "data_train = pd.read_csv('twitter_training.csv', encoding='latin-1')\n",
        "data_test = pd.read_csv('twitter_test.csv', encoding='latin-1')\n",
        "data_validation = pd.read_csv('twitter_validation.csv', encoding='latin-1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrgcDRnxsint",
        "outputId": "bfb1475b-5bde-472a-e87f-1ace2ee87103"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if isinstance(text, str): # Check if text is a string\n",
        "        # Remove special characters and numbers\n",
        "        text = re.sub(r'[^a-zA-Zآ-ی]', ' ', text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Tokenize the text\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        tokens = [token for token in tokens if token not in stopwords_list]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "data_train['Tweet content'] = data_train['Tweet content'].apply(preprocess_text)\n",
        "data_test['Tweet content'] = data_test['Tweet content'].apply(preprocess_text)\n",
        "data_validation['Tweet content'] = data_validation['Tweet content'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "ClB3dBy9s6US"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(data_train['sentiment'])\n",
        "y_test = label_encoder.transform(data_test['sentiment'])\n",
        "y_validation = label_encoder.transform(data_validation['sentiment'])"
      ],
      "metadata": {
        "id": "nFCX-3FctD6z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "data_train['Tweet content'].fillna('', inplace=True)\n",
        "data_test['Tweet content'].fillna('', inplace=True)\n",
        "data_validation['Tweet content'].fillna('', inplace=True)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(data_train['Tweet content'])\n",
        "X_test = vectorizer.transform(data_test['Tweet content'])\n",
        "X_validation = vectorizer.transform(data_validation['Tweet content'])"
      ],
      "metadata": {
        "id": "rnvAZpcjtP4g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "A5CXmxIhtg70"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ...\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# ...\n",
        "\n",
        "X_train_dense = X_train.toarray()\n",
        "X_val_dense = X_val.toarray()\n",
        "\n",
        "# ...\n",
        "\n",
        "history = model.fit(X_train_dense, y_train, batch_size=64, epochs=10, validation_data=(X_val_dense, y_val))"
      ],
      "metadata": {
        "id": "zU1tOu7gtk7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "cYa3R7wTvy-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict_classes(X_test)"
      ],
      "metadata": {
        "id": "vlXbVcqFxkOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_sentiments = label_encoder.inverse_transform(predictions.flatten())\n",
        "data_test['Predicted Sentiment'] = predicted_sentiments\n",
        "data_test.to_excel('neural_network_predictions.xlsx', index=False)"
      ],
      "metadata": {
        "id": "kFyeFp4yxmL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}