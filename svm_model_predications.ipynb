{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Input Datasets and Preprocessing**\n",
        "\n",
        "in this part of code, first we have three inputs of datasets training, testing and evaluating.\n",
        "then we have preprocessing module such with these functionality:\n",
        "\n",
        "\n",
        "1.   Removing special character in english and persian\n",
        "2.   Converting to lowercase\n",
        "3.   tokenizing texts\n",
        "4.   removing stopwords\n",
        "5.   lemmitization\n",
        "6.   Joining the tokens back into a single string\n",
        "\n",
        "then we have outputs of these head of dataset after preprocessing\n"
      ],
      "metadata": {
        "id": "Zw2UljmwOOy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeOq5ExxLX27",
        "outputId": "8e16d848-0e01-423f-afb1-f362f6570dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Tweet ID       entity sentiment                 Tweet content\n",
            "0      2401  Borderlands  Positive  im getting borderland murder\n",
            "1      2401  Borderlands  Positive            coming border kill\n",
            "2      2401  Borderlands  Positive    im getting borderland kill\n",
            "3      2401  Borderlands  Positive   im coming borderland murder\n",
            "4      2401  Borderlands  Positive  im getting borderland murder\n",
            "   Tweet ID     entity   sentiment  \\\n",
            "0      3364   Facebook  Irrelevant   \n",
            "1       352     Amazon     Neutral   \n",
            "2      8312  Microsoft    Negative   \n",
            "3      4371      CS-GO    Negative   \n",
            "4      4433     Google     Neutral   \n",
            "\n",
            "                                       Tweet content  \n",
            "0  mentioned facebook struggling motivation go ru...  \n",
            "1  bbc news amazon bos jeff bezos reject claim co...  \n",
            "2  microsoft pay word function poorly samsungus c...  \n",
            "3  csgo matchmaking full closet hacking truly awf...  \n",
            "4  president slapping american face really commit...  \n",
            "   Tweet ID                             entity sentiment  \\\n",
            "0      5328                        Hearthstone  Negative   \n",
            "1      7618                          MaddenNFL  Negative   \n",
            "2      7108                    johnson&johnson  Negative   \n",
            "3     10008  PlayerUnknownsBattlegrounds(PUBG)  Negative   \n",
            "4        49                             Amazon   Neutral   \n",
            "\n",
            "                                       Tweet content  \n",
            "0  blizzardcs going hearthstone ipad deleted app ...  \n",
            "1  eamaddennfl reason offline franchise lag much ...  \n",
            "2  johnson johnson enter phase trial covid vaccin...  \n",
            "3  banning pubg going fix anything gon na make ev...  \n",
            "4  played interesting quiz amazon try luck chance...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download nltk resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Load the data\n",
        "data_train = pd.read_csv('twitter_training.csv', encoding='latin-1')\n",
        "data_test = pd.read_csv('twitter_test.csv', encoding='latin-1')\n",
        "data_validation = pd.read_csv('twitter_validation.csv', encoding='latin-1')\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        # Remove special characters and numbers\n",
        "        text = re.sub(r'[^a-zA-Zآ-ی]', ' ', text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Tokenize the text\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        tokens = [token for token in tokens if token not in stopwords_list]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "data_train['Tweet content'] = data_train['Tweet content'].apply(preprocess_text)\n",
        "data_test['Tweet content'] = data_test['Tweet content'].apply(preprocess_text)\n",
        "data_validation['Tweet content'] = data_validation['Tweet content'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "print(data_train.head())\n",
        "print(data_test.head())\n",
        "print(data_validation.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**\n",
        "\n",
        "\n",
        "In this part of code, we have Feature selection operation. and First, we should Replace NaN values with an empty string. second, we should implement the operation of feature selection and extraction using TF-IDF.\n",
        "\n",
        "we use TfidfVectorizer from scikit-learn to convert the preprocessed text into numerical features. We fit the vectorizer on the training data and transform the test and validation data using the fitted vectorizer. We also store the corresponding sentiment labels in separate variables.\n",
        "\n",
        "in output of this part we can see this vectorization..."
      ],
      "metadata": {
        "id": "XaBWIrhnQTM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Replace NaN values with an empty string\n",
        "data_train['Tweet content'].fillna('', inplace=True)\n",
        "data_test['Tweet content'].fillna('', inplace=True)\n",
        "data_validation['Tweet content'].fillna('', inplace=True)\n",
        "\n",
        "# Feature Extraction\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(data_train['Tweet content'])\n",
        "X_test = vectorizer.transform(data_test['Tweet content'])\n",
        "X_validation = vectorizer.transform(data_validation['Tweet content'])\n",
        "\n",
        "y_train = data_train['sentiment']\n",
        "y_test = data_test['sentiment']\n",
        "y_validation = data_validation['sentiment']\n",
        "\n",
        "print (X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rneSVXZPiSN",
        "outputId": "ef1dd5f6-a12c-456e-e894-3df3b80f1588"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 15143)\t0.6544346822104361\n",
            "  (0, 2604)\t0.4074132771999723\n",
            "  (0, 9148)\t0.42357797560585864\n",
            "  (0, 11123)\t0.47572194280161867\n",
            "  (1, 12633)\t0.501903248747078\n",
            "  (1, 2601)\t0.6902302356270013\n",
            "  (1, 4201)\t0.521224856202602\n",
            "  (2, 12633)\t0.5399671738206703\n",
            "  (2, 2604)\t0.45351894519199304\n",
            "  (2, 9148)\t0.4715129512311874\n",
            "  (2, 11123)\t0.5295578857587865\n",
            "  (3, 4201)\t0.48600512487910713\n",
            "  (3, 15143)\t0.6313859910022002\n",
            "  (3, 2604)\t0.3930644917893328\n",
            "  (3, 11123)\t0.45896737820002736\n",
            "  (4, 15143)\t0.6544346822104361\n",
            "  (4, 2604)\t0.4074132771999723\n",
            "  (4, 9148)\t0.42357797560585864\n",
            "  (4, 11123)\t0.47572194280161867\n",
            "  (5, 15143)\t0.6544346822104361\n",
            "  (5, 2604)\t0.4074132771999723\n",
            "  (5, 9148)\t0.42357797560585864\n",
            "  (5, 11123)\t0.47572194280161867\n",
            "  (6, 12060)\t0.250620536967174\n",
            "  (6, 25500)\t0.2898143722283676\n",
            "  :\t:\n",
            "  (74679, 11001)\t0.27102604405314906\n",
            "  (74679, 26307)\t0.19824373582358248\n",
            "  (74680, 16812)\t0.41140948118007103\n",
            "  (74680, 15912)\t0.19642521538823637\n",
            "  (74680, 6456)\t0.27944591398588514\n",
            "  (74680, 25673)\t0.2639957189023763\n",
            "  (74680, 15789)\t0.3192151903236705\n",
            "  (74680, 3254)\t0.2660646389104806\n",
            "  (74680, 18755)\t0.31808724329029064\n",
            "  (74680, 13849)\t0.3234337697172804\n",
            "  (74680, 1978)\t0.27336469773896754\n",
            "  (74680, 11001)\t0.247189107965639\n",
            "  (74680, 7374)\t0.1977798598735336\n",
            "  (74680, 8715)\t0.187464034618537\n",
            "  (74680, 26307)\t0.1808080562486363\n",
            "  (74680, 13322)\t0.1520397449515146\n",
            "  (74681, 16812)\t0.4644809379542444\n",
            "  (74681, 6456)\t0.31549418808565\n",
            "  (74681, 25673)\t0.29805093159243556\n",
            "  (74681, 15789)\t0.36039366566247083\n",
            "  (74681, 13849)\t0.36515643804184805\n",
            "  (74681, 1978)\t0.30862850035728534\n",
            "  (74681, 11001)\t0.2790762828086099\n",
            "  (74681, 26307)\t0.2041321345224226\n",
            "  (74681, 13322)\t0.3433054733636238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**\n",
        "\n",
        "In this part, we create instances of the Naive Bayes (MultinomialNB) and SVM (SVC) models. We then train these models using the preprocessed training data and the corresponding sentiment labels.\n",
        "\n",
        "so we used two method of training: 1. Naive bayes  2. SVM\n",
        "\n",
        "these are two important methods for classinfication modeling\n",
        "\n",
        "and fitting with svm is so long time and complexity...\n",
        "\n",
        "\n",
        "but fitting with naive bayes is so fast"
      ],
      "metadata": {
        "id": "Dn53iUw6SA_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Model Training\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train, y_train)  # Fit the SVM model with the training data\n",
        "\n",
        "svm_predictions = svm_model.predict(X_test)  # Make predictions using the trained model\n",
        "\n",
        "# Evaluation\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "print(\"SVM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, svm_predictions))"
      ],
      "metadata": {
        "id": "5twTH2NbWPRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving Predictions to Excel**\n",
        "\n",
        "In this part, we define a function save_predictions() that takes the predicted sentiment labels, the original data, and a filename as input. The function creates a copy of the data, adds a new columncalled \"Predicted Sentiment\" containing the predictions, and saves the modified data to an Excel file using to_excel(). We then call this function for both Naive Bayes and SVM predictions, providing the predictions, test data, and desired filenames."
      ],
      "metadata": {
        "id": "eCOX_C4xYNAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(predictions, data, filename):\n",
        "    data_with_predictions = data.copy()\n",
        "    data_with_predictions['Predicted Sentiment'] = predictions\n",
        "    data_with_predictions.to_excel(filename, index=False)\n",
        "\n",
        "save_predictions(svm_predictions, data_test, 'svm_predictions.xlsx')"
      ],
      "metadata": {
        "id": "rAubonKgYEGn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c08b145d-e9e2-49ec-f57f-122144cdc76b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ed48c454fe2f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_with_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msave_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'svm_predictions.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'svm_predictions' is not defined"
          ]
        }
      ]
    }
  ]
}